info gain and correlation attribute orderings are fairly similar. backwards
wrapper matches those rankings more than forwards wrapper (I think--didn't look
too closely).

mlp had worst precision, decision tree had best. naive bayes had best f1 score,
but we don't care as much about recall. good example of why f1 score doesn't fit
our needs.

pca file looks interesting but probably needs a fair amount of time to analyze.
There might be interesting things in there.

How does weka's wrapper algorithms choose when to stop adding/removing features?
is it whenever the metric stops improving? Seems unlikely because the decision
tree with all the features has a higher f1 score than the decision trees using
subsets.

subset_wrapperforward/j48.txt has the highest precision.

Seems like it'd be nice to have a wrapper algorithm that optimizes f0.2 score or
something like that.

TODO:
based on attribute selection output, make one or more subset data sets. rerun
all the models on them. analyze.
create step-by-step procedure (e.g. for use in the speeches team) for how to
evaluate the helpfullness of a new feature/whether or not to keep it.
